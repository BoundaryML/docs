---
title: "Quickstart"
description: Start building with Gloo in 5 minutes
---

Gloo allows developers to build, test and deploy powerful LLM apps using a task-based (aka "function-based") architecture.

In order for any chatbot or LLM-powered app to interface with your existing code, you need to be able to mold its output into a structured format your program can understand.

<Tip>
  An AI agent that uses "tools" is really just performing a classification task.
  The input is a question, and the output is a list of tools. This is just one
  example. Gloo allows you to define tasks like these in very simple steps, and
  in the future, use the LLM-generated data to train specialized models.
</Tip>

Gloo consists of the following parts:

1. **Gloo CLI**
2. **Gloo Task Definitions**
3. **Gloo Dashboard + Data** warehouse to view tests, or query your data to train your own models.

## Building a semantic search task

In this guide we will walk through how you can build a semantic search task using Gloo.

#### Prerequisites:

1. Install [Node + NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)
1. Install npx: `npm install -g npx`, which can run the Gloo CLI without installing it globally
1. Run `npx @gloo-ai/client --version`, and if you see the version you're now ready to go.

### Step 1: Add a tasks.yaml file to your project.

The `tasks.yaml` is a definition of all, or a subset of tasks you want your program to perform using generative models. You can think of each task as the equivalent of an LLM prompt with a specific instruction and an output schema.

```yaml project/tasks/tasks.yaml
tasks:
  SearchDocuments:
    input: SearchInput
    output: SearchOutput

types:
  ## Search
  SearchInput:
    query: string
    context: string
  SearchOutput:
    clues: list<string>
    reasoning: string
    answer: string
```

### Generate your Task Definitions

Next we will use the **Gloo CLI** to generate your task definitions with this command:

```bash
npx @gloo-ai/client generate python --yaml tasks/tasks.yaml
```

This will generate a few files in your project:

```bash
tasks/
  task_search_documents/
    v0/
      generated.py
      task.py
      test.py # Coming soon
```

### Edit your generated Task Definition

Now we can edit the generated `task.py`. At a glance, you will see a lot of commented out functions.
Below we go over each of these functions and what they do.

<Card title="Task.py" icon="lightbulb" iconType="duotone" color="#0aeb04">

In general, view the `tasks.yaml` as a definition of _what_ tasks are, and the generated `task.py` files as definitions for _how_ they are executed. In the case of LLM apps, the "how" is done using prompt engineering and a combination of different LLM model parameters like temperature, etc. You can see both are defined in the next example.

</Card>

#### Modify the prompt

We will leverage the input variables defined in `task.yaml` schema to build our prompt, and we will ensure the output instructions can output the schema we desire.

Recall that for our **Search** task the desired output schema is:

```yaml
SearchOutput:
  clues: list<string>
  reasoning: string
  answer: string
```

Which translates to this JSON:

```json
{
  "clues": string[],
  "reasoning": string,
  "answer": string,
}
```

Now we can edit the prompt and ensure our task actually spits out that output.

```python
  class search(GlooLLMTaskInterface):
     # ...

      def edit_llm_client(self) -> LLMClient:
        return OpenAILLMClient(model_name="gpt-3.5-turbo", temperature=0)

      def edit_prompt_template(self) -> str:
          return f"""
          Answer the question given using the following context:

          Context: {VARS.in_SearchInput.context}
          Question: {VARS.in_SearchInput.query}

          Respond in the following format:
          {
            "clues": string[],
            "reasoning": string,
            "answer": string,
          }

          JSON:
          """
     # ...

  async def run_v0(*, input: SearchInputModel) -> SearchOutputModel:
      task = search()
      return await task.run(input)
```

Note how we were able to refer to variables defined in the `task.yaml` **SearchInput** fields. These are all automatically generated, and as you guessed it, the output format can also be generated using your generated **SearchOutput** schema fields. This will generate the same text as above:

```string
...
Output in the following format:

{
  "{VARS.out_SearchOutput.clues}": string[],
  "{VARS.out_SearchOutput.reasoning}": string,
  "{VARS.out_SearchOutput.answer}": string,
}
...
```

All of these variable substitutions are tracked by Gloo and can be tracked individually in the dashboard. Gloo will let you use this data to dive deep into potential issues caused by specific sections of your prompt. I.e. you could narrow down which part of your prompt causes the most impact in generating good or bad responses.
-- add dashboard link here --

### Run the task

```Python
from tasks.task_search_documents import run_search_documents
import asyncio

def run_search()
  context = perform_semantic_search(query)
  input = SearchInputModel(
    query="What is the capital of France?",
    context={context},
  )
  # Note you can run synchronous version by importing run_search_documents_sync instead.
  output = asyncio.run(run_search_documents(input=input))
  print(output)
```

### Advanced prompt engineering

Recall the simple schema we have:

```
  {
    "clues": string[],
    ...
  }
```

Using `string[]` may not be the best description for `clues`. For example, you may want to indicate what kind of clues you want the model to actually output. Maybe you only want sentiment-based clues, or entities, or facts. We need something more than just declaring the type of `clues`. To do this, there a a couple of ways.

#### 1. Add a "clues" instruction using a prompt template variable

Instructions can be provided at the top of the prompt. Depending on the LLM model, performance may change by adding these at the beginning, rather than within the schema itself.

To experiment with this... -TODO-

#### 2. Use the Output -> JSON converter to define a custom description.

Gloo comes with a built-in function called `edit_types_json_converter` that allows you to define the output format "description" -- in this case it returns a **`search__Description`**. Here we can define more custom descriptions for what `clues` means that goes beyond the simple type of this field.

In the example below, we declare a custom description for `clues` that changes the name to "hints", and refer to it in the prompt definition. Note that even though we changed the name of this field, from a programmatic POV you will still get a model output with a field containing `output.clues`.

```python
class search(...)
  def edit_types_json_converter(self) -> search__Description:
    return {
      "SearchOutput": {
          "clues": {
              "name": "hints",
              "description": "string[], where each clue is an entity that will help answer the question in a factual way",
          },
          ... other fields search output ...
      },
    }

  def edit_prompt_template()
    return f'''
    Answer the question given using the following context:

    Context: {VARS.in_SearchInput.context}
    Question: {VARS.in_SearchInput.query}

    Respond in the following format:
    {
      "{VARS.out_SearchOutput.clues}": {VARS.out_SearchOutput.clues.desc},
      "reasoning": string,
      "answer": string,
    }

    JSON:
    '''
```

This will generate a prompt string that looks like:

```string
...
{
  "hints": string[], where each clue is an entity that will help answer the question in a factual way,
  "reasoning": string,
  "answer": string,
}
...
```
